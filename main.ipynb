{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":482,"sourceType":"datasetVersion","datasetId":228},{"sourceId":5344155,"sourceType":"datasetVersion","datasetId":3102947}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/diabetes-prediction?scriptVersionId=269770477\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:40:55.167541Z","iopub.execute_input":"2025-10-21T18:40:55.167838Z","iopub.status.idle":"2025-10-21T18:40:55.180111Z","shell.execute_reply.started":"2025-10-21T18:40:55.167792Z","shell.execute_reply":"2025-10-21T18:40:55.179372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pip install -U imbalanced-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Import Necessary Libraries ---\nprint(\"--- 1. Importing Libraries ---\")\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Preprocessing and Feature Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# Metrics and Imbalance Handling\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\nfrom imblearn.over_sampling import SMOTE\n\n# Model Interpretation\nimport shap\nimport lime\nimport lime.lime_tabular\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\nprint(\"Libraries imported successfully.\\n\" + \"=\"*80)\n\n# --- 2. Load and Explore Primary Dataset ---\nprint(\"\\n--- 2. Loading and Exploring the Primary Dataset ---\")\ntry:\n    df_primary = pd.read_csv('/kaggle/input/diabetes-prediction-dataset/diabetes_prediction_dataset.csv')\nexcept FileNotFoundError:\n    print(\"Error: 'diabett.csv' not found. Please ensure the dataset is in the same directory.\")\n    exit()\n\nprint(\"Primary dataset loaded. Shape:\", df_primary.shape)\nprint(df_primary.head())\ndf_primary.info()\n\n# Handle the 'Other' category in gender by removing it for simplicity\ndf_primary = df_primary[df_primary['gender'] != 'Other'].copy()\nprint(\"\\n'Other' gender category removed.\")\nprint(\"\\n\" + \"=\"*80)\n\n# --- 3. Feature Engineering on Primary Dataset ---\nprint(\"\\n--- 3. Performing Feature Engineering ---\")\n# a) Binning BMI into categories\nbins_bmi = [0, 18.5, 24.9, 29.9, 100]\nlabels_bmi = ['Underweight', 'Normal', 'Overweight', 'Obese']\ndf_primary['bmi_category'] = pd.cut(df_primary['bmi'], bins=bins_bmi, labels=labels_bmi)\n\n# b) Binning Age into groups\nbins_age = [0, 12, 19, 39, 59, 100]\nlabels_age = ['Child', 'Teenager', 'Adult', 'Middle_Aged', 'Senior']\ndf_primary['age_group'] = pd.cut(df_primary['age'], bins=bins_age, labels=labels_age)\n\nprint(\"New categorical features 'bmi_category' and 'age_group' created.\")\nprint(df_primary[['age', 'age_group', 'bmi', 'bmi_category']].head())\nprint(\"\\n\" + \"=\"*80)\n\n# --- 4. Data Preparation for Modeling ---\nprint(\"\\n--- 4. Preparing Data in Different Formats for Modeling ---\")\nX = df_primary.drop('diabetes', axis=1)\ny = df_primary['diabetes']\n\n# Data with one-hot encoding for all models\ncategorical_features = ['gender', 'smoking_history', 'bmi_category', 'age_group']\nX_one_hot = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n\nprint(\"Data preparation complete.\")\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:37:34.486369Z","iopub.execute_input":"2025-10-21T17:37:34.486617Z","iopub.status.idle":"2025-10-21T17:37:41.944914Z","shell.execute_reply.started":"2025-10-21T17:37:34.486597Z","shell.execute_reply":"2025-10-21T17:37:41.944239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Data Splitting and SMOTE ---\nprint(\"\\n--- 5. Splitting Data and Handling Class Imbalance with SMOTE ---\")\nX_train_oh, X_test_oh, y_train, y_test = train_test_split(X_one_hot, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Original training data distribution:\\n{y_train.value_counts(normalize=True)}\")\n\n# Apply SMOTE to the numerical (one-hot encoded) training data.\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_oh, y_train)\n\nprint(f\"\\nSMOTE-balanced training data distribution:\\n{y_train_smote.value_counts(normalize=True)}\")\nprint(\"\\n\" + \"=\"*80)\n\n# --- 6. Model Training and Hyperparameter Tuning ---\nprint(\"\\n--- 6. Training and Tuning Models on Primary Dataset ---\")\nmodels_to_tune = {\n    \"RandomForest\": (RandomForestClassifier(random_state=42), {\n        'n_estimators': [100, 200], 'max_depth': [10, 20], 'min_samples_split': [2, 5]\n    }),\n    \"XGBoost\": (XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', tree_method='gpu_hist'), {\n        'n_estimators': [100, 200], 'max_depth': [3, 5], 'learning_rate': [0.05, 0.1]\n    })\n}\nbest_estimators = {}\nfor name, (model, params) in models_to_tune.items():\n    print(f\"\\nTuning {name}...\")\n    search = RandomizedSearchCV(model, params, n_iter=4, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1, verbose=1)\n    search.fit(X_train_smote, y_train_smote)\n    best_estimators[name] = search.best_estimator_\n    print(f\"Best parameters for {name}: {search.best_params_}\")\n\nprint(\"\\nTraining SVM (on CPU)...\")\nsvm_pipeline = Pipeline([('scaler', StandardScaler()), ('svm', SVC(kernel='rbf', probability=True, random_state=42))])\nsvm_pipeline.fit(X_train_smote, y_train_smote)\nbest_estimators[\"SVM\"] = svm_pipeline\n\nprint(\"\\nTraining CatBoost (on GPU)...\")\ncat_model = CatBoostClassifier(random_state=42, verbose=0, task_type='GPU')\ncat_model.fit(X_train_smote, y_train_smote)\nbest_estimators[\"CatBoost\"] = cat_model\nprint(\"\\n\" + \"=\"*80)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:02:29.618444Z","iopub.execute_input":"2025-10-21T18:02:29.618762Z","iopub.status.idle":"2025-10-21T18:23:16.682266Z","shell.execute_reply.started":"2025-10-21T18:02:29.618741Z","shell.execute_reply":"2025-10-21T18:23:16.681467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 8. Final Model Evaluation on Primary Test Set ---\nprint(\"\\n--- 8. Final Evaluation on Primary Dataset's Test Set ---\")\nroc_data = {}\nfor name, model in best_estimators.items():\n    print(f\"\\n--- {name} Performance ---\")\n    y_pred = model.predict(X_test_oh)\n    y_proba = model.predict_proba(X_test_oh)[:, 1]\n\n    roc_data[name] = (y_proba, y_test)\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    print(f\"ROC AUC Score: {auc(fpr, tpr):.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred))\n\n# --- ROC Curve Comparison ---\nplt.figure(figsize=(12, 10))\nfor name, (y_proba, y_true) in roc_data.items():\n    fpr, tpr, _ = roc_curve(y_true, y_proba)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curve Comparison on Primary Dataset', fontsize=16)\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\nprint(\"\\n\" + \"=\"*80)\n\n\n# --- 9. Model Interpretation (SHAP & LIME) ---\nprint(\"\\n--- 9. Interpreting the Best Model (XGBoost) ---\")\n# SHAP for Global Interpretation\ntry:\n    best_xgb_model = best_estimators['XGBoost']\n    best_xgb_model.set_params(predictor='cpu_predictor')\n\n    explainer = shap.TreeExplainer(best_xgb_model)\n    shap_values = explainer.shap_values(X_test_oh)\n\n    print(\"\\nSHAP Feature Importance:\")\n    shap.summary_plot(shap_values, X_test_oh, plot_type=\"bar\", show=False)\n    plt.title(\"SHAP Feature Importance (XGBoost)\")\n    plt.show()\nexcept Exception as e:\n    print(f\"Could not generate SHAP plots. Error: {e}\")\n\n# LIME for Local Interpretation\ntry:\n    predict_fn_xgb = lambda x: best_estimators['XGBoost'].predict_proba(x)\n\n    categorical_names = {}\n    categorical_features_indices = []\n    for i, col in enumerate(X_train_smote.columns):\n        if '_' in col:\n            original_feature = col.split('_')[0]\n            if original_feature not in categorical_names:\n                categorical_names[original_feature] = []\n            categorical_names[original_feature].append(i)\n            categorical_features_indices.append(i)\n\n    explainer = lime.lime_tabular.LimeTabularExplainer(\n        training_data=X_train_smote.values,\n        feature_names=X_train_smote.columns.tolist(),\n        class_names=['Not Diabetic', 'Diabetic'],\n        categorical_features=categorical_features_indices,\n        categorical_names=categorical_names,\n        mode='classification'\n    )\n    true_positives = X_test_oh[(y_test == 1) & (best_estimators['XGBoost'].predict(X_test_oh) == 1)]\n    if not true_positives.empty:\n        instance_to_explain_tp = true_positives.iloc[0]\n        print(\"\\nLIME Explanation for a True Positive case:\")\n        explanation_tp = explainer.explain_instance(instance_to_explain_tp.values, predict_fn_xgb, num_features=10)\n        explanation_tp.as_pyplot_figure()\n        plt.suptitle(\"LIME: True Positive Explanation\", y=1.02)\n        plt.show()\nexcept Exception as e:\n    print(f\"Could not generate LIME plots. Error: {e}\")\nprint(\"\\n\" + \"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:33:38.889234Z","iopub.execute_input":"2025-10-21T18:33:38.889522Z","iopub.status.idle":"2025-10-21T18:34:33.348851Z","shell.execute_reply.started":"2025-10-21T18:33:38.889501Z","shell.execute_reply":"2025-10-21T18:34:33.348077Z"}},"outputs":[],"execution_count":null}]}